{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "from cuda import cudart\n",
    "import common\n",
    "from image_batcher import ImageBatcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"EngineBuilder\").setLevel(logging.INFO)\n",
    "log = logging.getLogger(\"EngineBuilder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngineCalibrator(trt.IInt8EntropyCalibrator2):\n",
    "    \"\"\"\n",
    "    Implements the INT8 Entropy Calibrator 2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cache_file):\n",
    "        \"\"\"\n",
    "        :param cache_file: The location of the cache file.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cache_file = cache_file\n",
    "        self.image_batcher = None\n",
    "        self.batch_allocation = None\n",
    "        self.batch_generator = None\n",
    "\n",
    "    def set_image_batcher(self, image_batcher: ImageBatcher):\n",
    "        \"\"\"\n",
    "        Define the image batcher to use, if any. If using only the cache file, an image batcher doesn't need\n",
    "        to be defined.\n",
    "        :param image_batcher: The ImageBatcher object\n",
    "        \"\"\"\n",
    "        self.image_batcher = image_batcher\n",
    "        size = int(np.dtype(self.image_batcher.dtype).itemsize *\n",
    "                   np.prod(self.image_batcher.shape))\n",
    "        self.batch_allocation = common.cuda_call(cudart.cudaMalloc(size))\n",
    "        self.batch_generator = self.image_batcher.get_batch()\n",
    "\n",
    "    def get_batch_size(self):\n",
    "        \"\"\"\n",
    "        Overrides from trt.IInt8EntropyCalibrator2.\n",
    "        Get the batch size to use for calibration.\n",
    "        :return: Batch size.\n",
    "        \"\"\"\n",
    "        if self.image_batcher:\n",
    "            return self.image_batcher.batch_size\n",
    "        return 1\n",
    "\n",
    "    def get_batch(self, names):\n",
    "        \"\"\"\n",
    "        Overrides from trt.IInt8EntropyCalibrator2.\n",
    "        Get the next batch to use for calibration, as a list of device memory pointers.\n",
    "        :param names: The names of the inputs, if useful to define the order of inputs.\n",
    "        :return: A list of int-casted memory pointers.\n",
    "        \"\"\"\n",
    "        if not self.image_batcher:\n",
    "            return None\n",
    "        try:\n",
    "            batch, _, _ = next(self.batch_generator)\n",
    "            log.info(\"Calibrating image {} / {}\".format(\n",
    "                self.image_batcher.image_index, self.image_batcher.num_images))\n",
    "            common.memcpy_host_to_device(\n",
    "                self.batch_allocation, np.ascontiguousarray(batch))\n",
    "            return [int(self.batch_allocation)]\n",
    "        except StopIteration:\n",
    "            log.info(\"Finished calibration batches\")\n",
    "            return None\n",
    "\n",
    "    def read_calibration_cache(self):\n",
    "        \"\"\"\n",
    "        Overrides from trt.IInt8EntropyCalibrator2.\n",
    "        Read the calibration cache file stored on disk, if it exists.\n",
    "        :return: The contents of the cache file, if any.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.cache_file):\n",
    "            with open(self.cache_file, \"rb\") as f:\n",
    "                log.info(\"Using calibration cache file: {}\".format(\n",
    "                    self.cache_file))\n",
    "                return f.read()\n",
    "\n",
    "    def write_calibration_cache(self, cache):\n",
    "        \"\"\"\n",
    "        Overrides from trt.IInt8EntropyCalibrator2.\n",
    "        Store the calibration cache to a file on disk.\n",
    "        :param cache: The contents of the calibration cache to store.\n",
    "        \"\"\"\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            log.info(\"Writing calibration cache data to: {}\".format(\n",
    "                self.cache_file))\n",
    "            f.write(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5836/2553814720.py:5: DeprecationWarning: Use set_memory_pool_limit instead.\n",
      "  config.max_workspace_size = 2**30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/18/2023-11:56:42] [TRT] [I] [MemUsageChange] Init CUDA: CPU +10, GPU +0, now: CPU 31, GPU 706 (MiB)\n",
      "[12/18/2023-11:56:48] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +227, GPU +34, now: CPU 334, GPU 725 (MiB)\n",
      "[12/18/2023-11:56:48] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger = trt.Logger(trt.Logger.INFO)\n",
    "trt.init_libnvinfer_plugins(logger, namespace=\"\")\n",
    "builder = trt.Builder(logger)\n",
    "config= builder.create_builder_config()\n",
    "config.max_workspace_size = 2**30\n",
    "network_flags = (1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "network = builder.create_network(network_flags)\n",
    "parser = trt.OnnxParser(network, logger)\n",
    "onnx_path = 'average_model.onnx'\n",
    "with open(onnx_path, \"rb\") as f:\n",
    "    parser.parse(f.read())\n",
    "inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
    "outputs = [network.get_output(i) for i in range(network.num_outputs)]\n",
    "for input in inputs:\n",
    "    batch_size = input.shape[0]\n",
    "config.set_flag(trt.BuilderFlag.STRICT_TYPES)\n",
    "config.set_flag(trt.BuilderFlag.FP16)\n",
    "config.set_flag(trt.BuilderFlag.INT8)\n",
    "calib_shape = [8] + list(inputs[0].shape[1:])\n",
    "calib_dtype = trt.nptype(inputs[0].dtype)\n",
    "img_batch = ImageBatcher('calibration/', calib_shape, calib_dtype, max_num_images=458, exact_batches=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator = EngineCalibrator('calibration.cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.int8_calibrator= calibrator\n",
    "config.int8_calibrator.set_image_batcher(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/18/2023-11:57:08] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[12/18/2023-11:57:08] [TRT] [W] Reshape_306: IShuffleLayer with zeroIsPlaceHolder=true has reshape dimension at position 1 that might or might not be zero. TensorRT resolves it at runtime, but this may cause excessive memory consumption and is usually a sign of a bug in the network.\n",
      "[12/18/2023-11:57:08] [TRT] [W] Reshape_345: IShuffleLayer with zeroIsPlaceHolder=true has reshape dimension at position 1 that might or might not be zero. TensorRT resolves it at runtime, but this may cause excessive memory consumption and is usually a sign of a bug in the network.\n",
      "[12/18/2023-11:57:08] [TRT] [W] Reshape_384: IShuffleLayer with zeroIsPlaceHolder=true has reshape dimension at position 1 that might or might not be zero. TensorRT resolves it at runtime, but this may cause excessive memory consumption and is usually a sign of a bug in the network.\n",
      "[12/18/2023-11:57:08] [TRT] [W] Reshape_306: IShuffleLayer with zeroIsPlaceHolder=true has reshape dimension at position 1 that might or might not be zero. TensorRT resolves it at runtime, but this may cause excessive memory consumption and is usually a sign of a bug in the network.\n",
      "[12/18/2023-11:57:08] [TRT] [W] Reshape_345: IShuffleLayer with zeroIsPlaceHolder=true has reshape dimension at position 1 that might or might not be zero. TensorRT resolves it at runtime, but this may cause excessive memory consumption and is usually a sign of a bug in the network.\n",
      "[12/18/2023-11:57:08] [TRT] [W] Reshape_384: IShuffleLayer with zeroIsPlaceHolder=true has reshape dimension at position 1 that might or might not be zero. TensorRT resolves it at runtime, but this may cause excessive memory consumption and is usually a sign of a bug in the network.\n",
      "[12/18/2023-11:57:08] [TRT] [I] Graph optimization time: 0.013597 seconds.\n",
      "[12/18/2023-11:57:08] [TRT] [W] BuilderFlag::kENABLE_TACTIC_HEURISTIC has been ignored in this builder run. This feature is only supported on Ampere and beyond.\n",
      "[12/18/2023-11:57:08] [TRT] [I] Timing cache disabled. Turning it on will improve builder speed.\n",
      "[12/18/2023-11:57:10] [TRT] [I] Detected 1 inputs and 2 output network tensors.\n",
      "[12/18/2023-11:57:12] [TRT] [I] Total Host Persistent Memory: 568368\n",
      "[12/18/2023-11:57:12] [TRT] [I] Total Device Persistent Memory: 3647488\n",
      "[12/18/2023-11:57:12] [TRT] [I] Total Scratch Memory: 3072000\n",
      "[12/18/2023-11:57:12] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 10 MiB, GPU 47 MiB\n",
      "[12/18/2023-11:57:12] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 346 steps to complete.\n",
      "[12/18/2023-11:57:12] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 132.342ms to assign 36 blocks to 346 nodes requiring 55374848 bytes.\n",
      "[12/18/2023-11:57:12] [TRT] [I] Total Activation Memory: 55374848\n",
      "[12/18/2023-11:57:12] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +56, now: CPU 0, GPU 106 (MiB)\n",
      "[12/18/2023-11:57:12] [TRT] [I] Starting Calibration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:EngineBuilder:Calibrating image 8 / 456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/18/2023-11:57:14] [TRT] [E] 1: [softMaxV2Runner.cpp::execute::226] Error Code 1: Cask (shader run failed)\n",
      "[12/18/2023-11:57:14] [TRT] [E] 3: [engine.cpp::~Engine::298] Error Code 3: API Usage Error (Parameter check failed at: runtime/api/engine.cpp::~Engine::298, condition: mExecutionContextCounter.use_count() == 1. Destroying an engine object before destroying the IExecutionContext objects it created leads to undefined behavior.\n",
      ")\n",
      "[12/18/2023-11:57:14] [TRT] [E] 2: [calibrator.cpp::calibrateEngine::1181] Error Code 2: Internal Error (Assertion context->executeV2(&bindings[0]) failed. )\n"
     ]
    }
   ],
   "source": [
    "engine = builder.build_serialized_network(network, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(engine)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
